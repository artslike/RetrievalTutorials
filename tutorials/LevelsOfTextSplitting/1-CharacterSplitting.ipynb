{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Levels Of Text Splitting**\n",
    "##  **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
    "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
    "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
    "* **Level 5: [Agentic Splitting](#AgenticChunking)** - Experimental method of splitting text with an agent-like system. Good for if you believe that token cost will trend to $0.00\n",
    "* **\\*Bonus Level:\\*** **[Alternative Representation Chunking + Indexing](#BonusLevel)** - Derivative representations of your raw text that will aid in retrieval and indexing\n",
    "\n",
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "\n",
    "Concepts to know:\n",
    "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (0.3.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yau/miniconda3/envs/docLoader/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the text I would like to ch',\n",
       " 'unk up. It is the example text for ',\n",
       " 'this exercise']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "chunk_size = 35\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i+chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to ch'),\n",
       " Document(metadata={}, page_content='unk up. It is the example text for '),\n",
       " Document(metadata={}, page_content='this exercise')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to ch'),\n",
       " Document(metadata={}, page_content='o chunk up. It is the example text'),\n",
       " Document(metadata={}, page_content='ext for this exercise')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=4, separator='')\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to'),\n",
       " Document(metadata={}, page_content='unk up. It is the example text for this exercise')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=4, separator='ch')\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../../data/PGEssays/mit.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='30b28634-431a-4c82-9ae3-8b2f64ea1d6f', embedding=None, metadata={'file_path': '../../data/PGEssays/mit.txt', 'file_name': 'mit.txt', 'file_type': 'text/plain', 'file_size': 36045, 'creation_date': '2024-12-14', 'last_modified_date': '2024-12-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9e933e6b-3157-47ca-90d8-c51272e1a50c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../../data/PGEssays/mit.txt', 'file_name': 'mit.txt', 'file_type': 'text/plain', 'file_size': 36045, 'creation_date': '2024-12-14', 'last_modified_date': '2024-12-14'}, hash='79fe2d2eb0ec7799dc5d882c29638bb6890e2d78e99d738238d48292d7b06968'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3dadae00-2f2d-4f5a-b45f-630e44e8af0d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5ebb6555924d31f20f1f5243ea3bfb18231fbb946cb76f497dbc73310fa36d3a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Want to start a startup?  Get funded by\\nY Combinator.\\n\\n\\n\\n\\nOctober 2006(This essay is derived from a talk at MIT.)\\nTill recently graduating seniors had two choices: get a job or go\\nto grad school.  I think there will increasingly be a third option:\\nto start your own startup.  But how common will that be?I'm sure the default will always be to get a job, but starting a\\nstartup could well become as popular as grad school.  In the late\\n90s my professor friends used to complain that they couldn't get\\ngrad students, because all the undergrads were going to work for\\nstartups.\", mimetype='text/plain', start_char_idx=2, end_char_idx=576, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docLoader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
